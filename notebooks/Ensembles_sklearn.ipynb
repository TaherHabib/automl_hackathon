{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ensembles_sklearn.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIBjnXDZ84ux",
        "outputId": "b2434362-2024-41ac-fb58-d27f6aa2fdc7"
      },
      "source": [
        "!pip uninstall --yes nvidia-ml-py3\n",
        "!pip uninstall --yes pandas-profiling\n",
        "!pip uninstall --yes scipy\n",
        "!pip install scipy==1.7.2\n",
        "!pip install pandas-profiling\n",
        "!pip install auto-sklearn==0.14.0"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: nvidia-ml-py3 7.352.0\n",
            "Uninstalling nvidia-ml-py3-7.352.0:\n",
            "  Successfully uninstalled nvidia-ml-py3-7.352.0\n",
            "Found existing installation: pandas-profiling 1.4.1\n",
            "Uninstalling pandas-profiling-1.4.1:\n",
            "  Successfully uninstalled pandas-profiling-1.4.1\n",
            "Found existing installation: scipy 1.4.1\n",
            "Uninstalling scipy-1.4.1:\n",
            "  Successfully uninstalled scipy-1.4.1\n",
            "Collecting scipy==1.7.2\n",
            "  Downloading scipy-1.7.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 38.2 MB 25 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from scipy==1.7.2) (1.19.5)\n",
            "Installing collected packages: scipy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 1.0.61 requires nvidia-ml-py3, which is not installed.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed scipy-1.7.2\n",
            "Collecting pandas-profiling\n",
            "  Downloading pandas_profiling-3.1.0-py2.py3-none-any.whl (261 kB)\n",
            "\u001b[K     |████████████████████████████████| 261 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting requests>=2.24.0\n",
            "  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n",
            "\u001b[K     |████████████████████████████████| 62 kB 890 kB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.7.2)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.19.5)\n",
            "Requirement already satisfied: missingno>=0.4.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.5.0)\n",
            "Collecting phik>=0.11.1\n",
            "  Downloading phik-0.12.0-cp37-cp37m-manylinux2010_x86_64.whl (675 kB)\n",
            "\u001b[K     |████████████████████████████████| 675 kB 45.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (4.62.3)\n",
            "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (1.1.5)\n",
            "Collecting multimethod>=1.4\n",
            "  Downloading multimethod-1.6-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: markupsafe~=2.0.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (2.0.1)\n",
            "Collecting joblib~=1.0.1\n",
            "  Downloading joblib-1.0.1-py3-none-any.whl (303 kB)\n",
            "\u001b[K     |████████████████████████████████| 303 kB 55.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (2.11.3)\n",
            "Collecting visions[type_image_path]==0.7.4\n",
            "  Downloading visions-0.7.4-py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 11.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: seaborn>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (0.11.2)\n",
            "Collecting htmlmin>=0.1.12\n",
            "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
            "Collecting PyYAML>=5.0.0\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 45.8 MB/s \n",
            "\u001b[?25hCollecting tangled-up-in-unicode==0.1.0\n",
            "  Downloading tangled_up_in_unicode-0.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 63.7 MB/s \n",
            "\u001b[?25hCollecting pydantic>=1.8.1\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 25.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from pandas-profiling) (3.2.2)\n",
            "Requirement already satisfied: networkx>=2.4 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (2.6.3)\n",
            "Requirement already satisfied: attrs>=19.3.0 in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (21.2.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (7.1.2)\n",
            "Collecting imagehash\n",
            "  Downloading ImageHash-4.2.1.tar.gz (812 kB)\n",
            "\u001b[K     |████████████████████████████████| 812 kB 69.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.0->pandas-profiling) (1.3.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling) (2018.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pydantic>=1.8.1->pandas-profiling) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=3.2.0->pandas-profiling) (1.15.0)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (2.0.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.24.0->pandas-profiling) (2.10)\n",
            "Requirement already satisfied: PyWavelets in /usr/local/lib/python3.7/dist-packages (from imagehash->visions[type_image_path]==0.7.4->pandas-profiling) (1.1.1)\n",
            "Building wheels for collected packages: htmlmin, imagehash\n",
            "  Building wheel for htmlmin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27098 sha256=a22fe89164a250d6d3a237d7bf45f007bef093a060a530d24504a8faa847fa95\n",
            "  Stored in directory: /root/.cache/pip/wheels/70/e1/52/5b14d250ba868768823940c3229e9950d201a26d0bd3ee8655\n",
            "  Building wheel for imagehash (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imagehash: filename=ImageHash-4.2.1-py2.py3-none-any.whl size=295207 sha256=33c572b4151eda1471e1577d576cb93a3d73b0c14ad2f887ab44c2b1ab160e76\n",
            "  Stored in directory: /root/.cache/pip/wheels/4c/d5/59/5e3e297533ddb09407769762985d134135064c6831e29a914e\n",
            "Successfully built htmlmin imagehash\n",
            "Installing collected packages: tangled-up-in-unicode, multimethod, visions, joblib, imagehash, requests, PyYAML, pydantic, phik, htmlmin, pandas-profiling\n",
            "  Attempting uninstall: joblib\n",
            "    Found existing installation: joblib 1.1.0\n",
            "    Uninstalling joblib-1.1.0:\n",
            "      Successfully uninstalled joblib-1.1.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 1.0.61 requires nvidia-ml-py3, which is not installed.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed PyYAML-6.0 htmlmin-0.1.12 imagehash-4.2.1 joblib-1.0.1 multimethod-1.6 pandas-profiling-3.1.0 phik-0.12.0 pydantic-1.8.2 requests-2.26.0 tangled-up-in-unicode-0.1.0 visions-0.7.4\n",
            "Collecting auto-sklearn==0.14.0\n",
            "  Downloading auto-sklearn-0.14.0.tar.gz (6.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.3 MB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.14.0) (57.4.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.14.0) (3.10.0.2)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.14.0) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.14.0) (1.7.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.14.0) (1.0.1)\n",
            "Collecting scikit-learn<0.25.0,>=0.24.0\n",
            "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: dask<2021.07 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.14.0) (2.12.0)\n",
            "Collecting distributed<2021.07,>=2.2.0\n",
            "  Downloading distributed-2021.6.2-py3-none-any.whl (722 kB)\n",
            "\u001b[K     |████████████████████████████████| 722 kB 56.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.14.0) (6.0)\n",
            "Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from auto-sklearn==0.14.0) (1.1.5)\n",
            "Collecting liac-arff\n",
            "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
            "Collecting threadpoolctl\n",
            "  Downloading threadpoolctl-3.0.0-py3-none-any.whl (14 kB)\n",
            "Collecting ConfigSpace<0.5,>=0.4.14\n",
            "  Downloading ConfigSpace-0.4.20-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2 MB 60.2 MB/s \n",
            "\u001b[?25hCollecting pynisher>=0.6.3\n",
            "  Downloading pynisher-0.6.4.tar.gz (11 kB)\n",
            "Collecting pyrfr<0.9,>=0.8.1\n",
            "  Downloading pyrfr-0.8.2-cp37-cp37m-manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 41.8 MB/s \n",
            "\u001b[?25hCollecting smac>=0.14\n",
            "  Downloading smac-1.1.1-py3-none-any.whl (208 kB)\n",
            "\u001b[K     |████████████████████████████████| 208 kB 35.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.14->auto-sklearn==0.14.0) (0.29.24)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from ConfigSpace<0.5,>=0.4.14->auto-sklearn==0.14.0) (2.4.7)\n",
            "Collecting cloudpickle>=1.5.0\n",
            "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn==0.14.0) (1.7.0)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn==0.14.0) (1.0.2)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn==0.14.0) (5.4.8)\n",
            "Requirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn==0.14.0) (2.4.0)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn==0.14.0) (2.0.0)\n",
            "Collecting dask<2021.07\n",
            "  Downloading dask-2021.6.2-py3-none-any.whl (973 kB)\n",
            "\u001b[K     |████████████████████████████████| 973 kB 56.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn==0.14.0) (5.1.1)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn==0.14.0) (7.1.2)\n",
            "Requirement already satisfied: toolz>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from distributed<2021.07,>=2.2.0->auto-sklearn==0.14.0) (0.11.1)\n",
            "Collecting fsspec>=0.6.0\n",
            "  Downloading fsspec-2021.11.0-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 57.9 MB/s \n",
            "\u001b[?25hCollecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->auto-sklearn==0.14.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->auto-sklearn==0.14.0) (2018.9)\n",
            "Collecting locket\n",
            "  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.0->auto-sklearn==0.14.0) (1.15.0)\n",
            "Collecting emcee>=3.0.0\n",
            "  Downloading emcee-3.1.1-py2.py3-none-any.whl (45 kB)\n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed<2021.07,>=2.2.0->auto-sklearn==0.14.0) (1.0.1)\n",
            "Building wheels for collected packages: auto-sklearn, pynisher, liac-arff\n",
            "  Building wheel for auto-sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for auto-sklearn: filename=auto_sklearn-0.14.0-py3-none-any.whl size=6585992 sha256=566d0bf038f228140cae2aeaef410476f8098d68deadc0e10e579f3ac5feb162\n",
            "  Stored in directory: /root/.cache/pip/wheels/c5/56/cc/e33d4a8cb4ffeb040d59ea08c4715d20806945dc80d3c25384\n",
            "  Building wheel for pynisher (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pynisher: filename=pynisher-0.6.4-py3-none-any.whl size=7044 sha256=2aa61c732c91c2d19cf3f30ef7a56b373a987b0fdf2ce4d3d893d04e3bb1a04e\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/71/95/7555ec3253e1ba8add72ae5febf1b015d297f3b73ba296d6f6\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11731 sha256=1fc703c973cc20bbb3de15808de385b72cd0950c5fe49e15e6e29ff70706603b\n",
            "  Stored in directory: /root/.cache/pip/wheels/1f/0f/15/332ca86cbebf25ddf98518caaf887945fbe1712b97a0f2493b\n",
            "Successfully built auto-sklearn pynisher liac-arff\n",
            "Installing collected packages: locket, partd, fsspec, cloudpickle, threadpoolctl, dask, scikit-learn, pyrfr, pynisher, emcee, distributed, ConfigSpace, smac, liac-arff, auto-sklearn\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Attempting uninstall: dask\n",
            "    Found existing installation: dask 2.12.0\n",
            "    Uninstalling dask-2.12.0:\n",
            "      Successfully uninstalled dask-2.12.0\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 1.25.3\n",
            "    Uninstalling distributed-1.25.3:\n",
            "      Successfully uninstalled distributed-1.25.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.0.0 which is incompatible.\u001b[0m\n",
            "Successfully installed ConfigSpace-0.4.20 auto-sklearn-0.14.0 cloudpickle-2.0.0 dask-2021.6.2 distributed-2021.6.2 emcee-3.1.1 fsspec-2021.11.0 liac-arff-2.5.0 locket-0.2.1 partd-1.2.0 pynisher-0.6.4 pyrfr-0.8.2 scikit-learn-0.24.2 smac-1.1.1 threadpoolctl-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-YYyxhK8md9"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UifIey7S72oZ",
        "outputId": "618568e0-fbf0-4e11-ed50-f250d0652a78"
      },
      "source": [
        "# https://drive.google.com/file/d/1GYV0XAVBgO95Lf-lTuMBf6YD2KaIjj8E/view?usp=sharing\n",
        "# https://drive.google.com/file/d/1eJRZdF1yf7a-CjAJUwTno6IaYzlKQChM/view?usp=sharing\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1GYV0XAVBgO95Lf-lTuMBf6YD2KaIjj8E' -O test.csv\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1eJRZdF1yf7a-CjAJUwTno6IaYzlKQChM' -O train.csv\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-11 12:20:04--  https://docs.google.com/uc?export=download&id=1GYV0XAVBgO95Lf-lTuMBf6YD2KaIjj8E\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.26.139, 74.125.26.101, 74.125.26.138, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.26.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0s-8o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/js2ks9jeq59obieh7ql6vhljfotfn8ci/1636633200000/18132256971849676264/*/1GYV0XAVBgO95Lf-lTuMBf6YD2KaIjj8E?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-11-11 12:20:06--  https://doc-0s-8o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/js2ks9jeq59obieh7ql6vhljfotfn8ci/1636633200000/18132256971849676264/*/1GYV0XAVBgO95Lf-lTuMBf6YD2KaIjj8E?e=download\n",
            "Resolving doc-0s-8o-docs.googleusercontent.com (doc-0s-8o-docs.googleusercontent.com)... 172.217.193.132, 2607:f8b0:400c:c03::84\n",
            "Connecting to doc-0s-8o-docs.googleusercontent.com (doc-0s-8o-docs.googleusercontent.com)|172.217.193.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 97603107 (93M) [text/csv]\n",
            "Saving to: ‘test.csv’\n",
            "\n",
            "test.csv            100%[===================>]  93.08M  82.7MB/s    in 1.1s    \n",
            "\n",
            "2021-11-11 12:20:08 (82.7 MB/s) - ‘test.csv’ saved [97603107/97603107]\n",
            "\n",
            "--2021-11-11 12:20:08--  https://docs.google.com/uc?export=download&id=1eJRZdF1yf7a-CjAJUwTno6IaYzlKQChM\n",
            "Resolving docs.google.com (docs.google.com)... 172.217.204.101, 172.217.204.113, 172.217.204.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|172.217.204.101|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0s-8o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/25rtgdlh8r147ori0qj98go62erk8mlq/1636633200000/18132256971849676264/*/1eJRZdF1yf7a-CjAJUwTno6IaYzlKQChM?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2021-11-11 12:20:09--  https://doc-0s-8o-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/25rtgdlh8r147ori0qj98go62erk8mlq/1636633200000/18132256971849676264/*/1eJRZdF1yf7a-CjAJUwTno6IaYzlKQChM?e=download\n",
            "Resolving doc-0s-8o-docs.googleusercontent.com (doc-0s-8o-docs.googleusercontent.com)... 172.217.193.132, 2607:f8b0:400c:c03::84\n",
            "Connecting to doc-0s-8o-docs.googleusercontent.com (doc-0s-8o-docs.googleusercontent.com)|172.217.193.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 73512474 (70M) [text/csv]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>]  70.11M  67.9MB/s    in 1.0s    \n",
            "\n",
            "2021-11-11 12:20:11 (67.9 MB/s) - ‘train.csv’ saved [73512474/73512474]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycOydO-I73v8"
      },
      "source": [
        "train_numData = pd.read_csv('train.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttKqdENn7jpY"
      },
      "source": [
        "def convertCategoricalsToNumerics(df):\n",
        "    \n",
        "    categorical_fields = df.select_dtypes(include=['object'])\n",
        "    categorical_fields = categorical_fields.drop('class',axis=1)\n",
        "    \n",
        "    for col in df.columns:\n",
        "        if col in categorical_fields.columns:\n",
        "                       \n",
        "            # Counting the different categories in the column\n",
        "            countOrdered_cats = list(dict(categorical_fields[col].value_counts()).keys()) # descending order\n",
        "            \n",
        "            \n",
        "            # Creating a list of numeric replacements\n",
        "            num_replacements = list(np.arange(len(countOrdered_cats))+1)\n",
        "            \n",
        "            # Dictionary of replacements to pass to df.replace()\n",
        "            replacements = dict(zip(countOrdered_cats, num_replacements))\n",
        "    \n",
        "            cd = df[col].replace(replacements, inplace=True)\n",
        "        \n",
        "        else:\n",
        "            pass\n",
        "        \n",
        "    return df\n",
        "    \n",
        "train_numData = convertCategoricalsToNumerics(train_data)\n",
        "train_numData"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4UUzJ4Dn7aiT"
      },
      "source": [
        "X = train_numData.loc[:, train_numData.columns != 'class']\n",
        "\n",
        "Y = train_numData.loc[:, train_numData.columns == 'class']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZPy2zbD7XDU"
      },
      "source": [
        "import autosklearn.classification\n",
        "import autosklearn.metrics\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "test_size = 0.2\n",
        "shuffle = True\n",
        "\n",
        "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=test_size, shuffle=shuffle)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BrSTD2lyfBfU"
      },
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "import ConfigSpace.hyperparameters as CSH\n",
        "\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "\n",
        "numeric_transformer = make_pipeline(StandardScaler())\n",
        "\n",
        "Preprocessor = ColumnTransformer(\n",
        " transformers=[\n",
        " ('numeric_transformer', numeric_transformer, train_X.select_dtypes(exclude=['object']).columns),\n",
        " ])\n",
        "\n",
        "hgb_pipe = make_pipeline(Preprocessor, HistGradientBoostingClassifier())\n",
        "\n",
        "search_space = {\n",
        "    'histgradientboostingclassifier__learning_rate': [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3],\n",
        "    'histgradientboostingclassifier__max_leaf_nodes': [None],\n",
        "    'histgradientboostingclassifier__min_samples_leaf': [10, 20, 50, 100],\n",
        "    'histgradientboostingclassifier__scoring': ['accuracy'],\n",
        "    'histgradientboostingclassifier__loss': ['categorical_crossentropy'],\n",
        "    'histgradientboostingclassifier__random_state': [53]\n",
        "}\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLgJ1P6_7TEU",
        "outputId": "d8fad7f0-39c8-4ef6-a420-3bcfeee098b9"
      },
      "source": [
        "\n",
        "#instantiate the Random CV Search\n",
        "hgb_grid = RandomizedSearchCV(hgb_pipe, search_space, \n",
        "                              n_jobs=-1, \n",
        "                              cv=5, \n",
        "                              scoring='accuracy',\n",
        "                              verbose=10, \n",
        "                              refit=True)\n",
        "\n",
        "hgb_grid.fit(train_X, train_Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
          ]
        }
      ]
    }
  ]
}